# -*- coding: utf-8 -*-
"""[FYP]Core logic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16-O2qap9MTMSPZUPom85p8DdhP2rC5tf
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

import numpy as np
import pandas as pd

# !ls /content/gdrive/MyDrive/fyp

user_weights = pd.read_csv('userWeights.csv')
user_feedback = pd.read_csv('CSP_QoS_UserFeedback1.csv')
csp_promised_parameters = pd.read_csv('CSP_Promised_Parameters (1).csv')

user_feedback

"""**algo1: TuCi**

1. User required weight inputs are multiplied with si values that CSPs proposed.

"""

print(csp_promised_parameters.shape)
csp_promised_parameters.describe

# ALGO1: TuCi values.
def calculate_tuci(user_inp, csp_params):
  return np.stack((csp_params[:,0], np.dot(csp_params[:,1:], user_inp.T)/100), axis =1)

inp1 = np.array([3,3,3,4])
csp_params = csp_promised_parameters.values
# Tuci = np.stack((csp_params[:,0], calculate_tuci(inp1,csp_params[:,1:])), axis=1)
# # len(Tuci)
# Tuci
calculate_tuci(inp1, csp_params)

"""## Collaborative filtering"""

from sklearn.metrics.pairwise import cosine_similarity
import operator
def similar_users(input_weights, other_users, k=3):
    # print(cosine_similarity(input_weights,other_users))
    # calc cosine similarity between user and each other user
    similarities = cosine_similarity(input_weights,other_users)[0].tolist()
    
    # create list of indices of these users
    indices = other_users.index.tolist()
    
    # create key/values pairs of user index and their similarity
    index_similarity = dict(zip(indices, similarities))
    
    # sort by similarity
    index_similarity_sorted = sorted(index_similarity.items(), key=operator.itemgetter(1))
    index_similarity_sorted.reverse()
    
    # grab k users off the top
    top_users_similarities = index_similarity_sorted[:k]
    users = [u[0] for u in top_users_similarities]
    # print(users)
    return top_users_similarities

input = np.array([3,3,3,4]).reshape(1,4)

def getlist(input, topk, user_feedback):
  csp_set = set()
  dummy = user_feedback.drop(["Cloud_Consumer_Name","Cloud_Service_Name", "Timestamp"],axis=1)
  # print(similar_users(input, dummy, topk))
  for k,v in similar_users(input, dummy, topk):
    csp_set.add((user_feedback.loc[k]['Cloud_Service_Name'],v, user_feedback.loc[k]['Timestamp'].strip()))
  return list(csp_set)
print(getlist(input, 20, user_feedback))

"""op: 
20 input
20 unique csp names
desired:
"csp name" - weighted avg score



now:
may not be 20 cspnames but 20 occurences in total.
"csp name" - n occuences.


"""

# pip install dateparser

import dateparser as dp
from sklearn import preprocessing as pp
from matplotlib import pyplot as plt 

def weighted_trust_per_csp(cllb_filter_output, delta):
    csp_set = set()
    for i in cllb_filter_output:
        csp_set.add(i[0])
    csp_set = list(csp_set)
    # print(csp_set)
    
    weighted_list = []
    for csp in csp_set:
      filter_csp=[]

      times = []
      for i in cllb_filter_output:
        if(i[0]==csp):
          filter_csp.append(i)
      if(len(filter_csp) != 1):
        for i in filter_csp:
          times.append(dp.parse(i[2]))
        # print(csp)
        # print(min(times), max(times))
        diminishing_factor=[]

        # print(filter_csp)
        # print(times)
        max_time = max(times)
        for i in range(len(times)):
          diminishing_factor.append(delta**((((max_time- times[i]).days)/365)/len(times)))

        # filter_csp.sort(key = lambda x : dp.parse(x[2]))
        temp = []
        for j in range(len(filter_csp)):
          temp.append(filter_csp[j][1])


        # # print(times)
        weighted_sum = 0
        # t2 = []
        # print(len(times), len(filter_csp))
        for i in range(len(filter_csp)):
          weighted_sum += filter_csp[i][1]*diminishing_factor[i]
          # t2.append(filter_csp[i][1]*diminishing_factor[i])
        weighted_list.append([csp,weighted_sum/len(filter_csp)])


        # x = range(len(filter_csp))
        # y = temp
        # x2 = range(len(filter_csp))
        # y2 = t2
        # plt.ylim(ymax = 1.5, ymin = 0)
        # plt.plot(x,y)
        # plt.plot(x2,y2)
        # plt.legend(["Dataset 1", "Dataset 2"])

        # plt.show()
      else:
        weighted_list.append([csp,filter_csp[0][1]])
    weighted_list = np.array(weighted_list)
    return weighted_list

# weighted_trust_per_csp(getlist(input,20, user_feedback) , 0.35)

def average_trust_per_csp(cllb_filter_output):
    csp_set = set()
    for i in cllb_filter_output:
        csp_set.add(i[0])
    csp_set = list(csp_set)
    # print(csp_set)

    weighted_list = []
    for csp in csp_set:
      filter_csp=[]
      scores = []
      
      for i in cllb_filter_output:
        if(i[0]==csp):
          filter_csp.append(i)
      
      if(len(filter_csp) != 1):
        # print(filter_csp)
        for i in filter_csp:
          scores.append(i[1])
        # print(csp)
        # print(min(scores), max(scores))
        weighted_list.append([csp, sum(scores)/len(scores)])
      else:
        weighted_list.append([csp,filter_csp[0][1]])

    weighted_list = np.array(weighted_list)
    return weighted_list

# weighted_trust_per_csp(getlist(input,20, user_feedback) , 0.5)
# average_trust_per_csp( getlist(input,20, user_feedback))

def merge_tuci_tfci(tfci, tuci, weights, topk):
  final_trust = []
  for i in range(len(tuci)):
    row =[]
    if(len(tfci)!= 0):
      row = tfci[np.where(tfci[:,0] == tuci[i][0])] 
    f = 0

    if(len(row)!=0):
      f = float(row[0][1])
    u = tuci[i][1]
    final_trust.append([tuci[i][0],u,f, weights[0]*u + weights[1]*f])
    final_trust.sort(key = lambda x : x[3], reverse = True)
  return final_trust[:topk]
  # print(tfci[np.where(tfci[:,0] == "Carbonite")])

# inp1 = np.array([3,3,3,4])
# ret = merge_tuci_tfci(weighted_trust_per_csp( getlist(inp1.reshape(1,4),20, user_feedback), 3.5), calculate_tuci(inp1, csp_params), [0.01, 0.99],10)
# ret[:10]

# inp1 = np.array([3,3,3,4])
# ret = merge_tuci_tfci(average_trust_per_csp( getlist(inp1.reshape(1,4),20, user_feedback)), calculate_tuci(inp1, csp_params), [0.01, 0.99],10)
# ret[:10]

# l = []
# for i in range(100):
#   l.append(i/100)

# mat = []
# for i in range(len(l)):
#   mat.append(merge_tuci_tfci(average_trust_per_csp( getlist(input,20,user_feedback)), calculate_tuci(inp1, csp_params), [l[i],1-l[i]],10)[0][-1]*100)  
# import numpy as np; np.random.seed(0)
# import seaborn as sns; sns.set_theme()

# import matplotlib.pyplot as plt
# print(mat)
# mat = np.asarray(mat).reshape(100,1)
# fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches
# sns.heatmap(mat, linewidths=.5, ax=ax)

# ax = sns.heatmap(mat)

# d = user_feedback.drop(1)
# user_feedback.values[1]

# inp1 = np.array([4,4,4,4])
# ret = merge_tuci_tfci(average_trust_per_csp( getlist(inp1.reshape(1,4),20)), calculate_tuci(inp1, csp_params), [0.01, 0.99])
# ret[:10]
feedback_db = user_feedback
weights = [0,0]
max_n = 0
w1 = [i/10 for i in range(1, 10)]
for x in range(len(w1)-1,-1,-1):
  avg_result= 0
  avg_max_result = 0
  match= 0
  for y in range(feedback_db.shape[0]):
    inp = feedback_db.values[y][1:5]
    dummy = feedback_db.drop(y)
    ret = merge_tuci_tfci(average_trust_per_csp(getlist(inp.reshape(1,4), 100, dummy)), calculate_tuci(inp, csp_params), [w1[x], 1-w1[x]],10)
    # print("ret")
    # print(ret)
    # print("avg list")
    # print(average_trust_per_csp(getlist(inp.reshape(1,4), 30, dummy)))
    # print("tuci")
    # print(calculate_tuci(inp, csp_params))

    ret = np.array(ret)
    # print("---"+feedback_db.values[y][5],ret)
    id = np.where(ret[:,0]==feedback_db.values[y][5])
    # print("id:")
    # print(id, ret[id])
    avg_max_result += float(ret[0][3])
    if(len(id[0]) == 0):
      avg_result+=0
    else:
    # print(w1[x] ,ret[id][0][3])
      if(feedback_db.values[y][5] in ret[:,0]):
        match+=1
      avg_result += float(ret[id][0][3])
  avg_result /= feedback_db.shape[0]
  avg_max_result /= feedback_db.shape[0]

  if(match > max_n):
    weights =[w1[x], 1-w1[x]]
    max_n = match
  print(w1[x],weights, avg_result, avg_max_result, match)

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

x = np.array(user_feedback["Uptime"])
y = np.array(user_feedback["Downtime"])
z = np.array(user_feedback["Fault_Tolerance_Capability"])
c = np.array(user_feedback["Application_Update_Frequency"])

img = ax.scatter(x, y, z, c=c, cmap=plt.hot())
fig.colorbar(img)
plt.show()

for i in csp_params[:,0]:
  data1 = user_feedback[user_feedback['Cloud_Service_Name']==i]
  uptime = data1['Uptime']
  downtime = data1['Downtime']
  ft = data1['Fault_Tolerance_Capability']
  auf = data1['Application_Update_Frequency']

  
  
  col = [uptime,downtime, ft,auf]
  fig, ax = plt.subplots()
  plt.title(i)
  ax.boxplot(col)
  plt.show()

print(data1['Uptime'])

ind = []
for i in csp_params[:,0]:
  data1 = user_feedback[user_feedback['Cloud_Service_Name']==i]
  Q1 = data1.quantile(0.25)
  Q3 = data1.quantile(0.75)
  IQR = Q3 - Q1

  count = (data1 < (Q1 - 1.5 * IQR)) |(data1 > (Q3 + 1.5 * IQR))
  count = count*1

  count["sum"] = count.sum(axis=1)
  for j,row in count.iterrows():
    if row['sum']>2:
      ind.append(j)

print(user_feedback.iloc[ind])

